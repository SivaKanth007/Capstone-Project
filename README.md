# ğŸ­ Data-Driven Anomaly Detection & Risk-Aware Maintenance Scheduling

> **FSE 570 Data Science Capstone** â€” Arizona State University, Ira A. Fulton Schools of Engineering

An end-to-end decision support system that transforms raw industrial sensor data into actionable maintenance recommendations using deep learning, Bayesian uncertainty quantification, and mathematical optimization.

## ğŸ‘¥ Team

| Name | Role |
|------|------|
| Anoushka Jaydas Dighe | Team Member |
| Deva Siva Kanth Tavvala | Team Member |
| Mohit Kumar Petla | Team Member |
| Umang Rajnikant Bid | Team Member |
| Urvansh Jignesh Shah | Team Member |

## ğŸ—ï¸ Architecture

```
Raw Sensor Data â†’ Preprocessing â†’ Anomaly Detection â†’ Risk Prediction â†’ MILP Optimization â†’ Dashboard
                  (LSTM Autoencoder)   (LSTM + XGBoost + Bayesian)    (PuLP Scheduler)     (Streamlit)
```

### Pipeline Components

| Component | Technique | Purpose |
|-----------|-----------|---------|
| **Anomaly Detection** | LSTM Temporal Autoencoder | Detect abnormal sensor patterns |
| **Failure Prediction** | LSTM Classifier + Attention | Failure probability within 30 cycles |
| **RUL Estimation** | XGBoost Regression | Remaining Useful Life estimation |
| **Uncertainty** | Bayesian Weibull Survival | Calibrated 90%/95% credible intervals |
| **Explainability** | SHAP + Attention Weights | Feature attribution & temporal importance |
| **Optimization** | MILP (PuLP CBC) | Crew-constrained maintenance scheduling |
| **Dashboard** | Streamlit + Plotly | Interactive fleet monitoring |

## ğŸ“Š Dataset

- **NASA C-MAPSS** â€” Turbofan engine degradation simulation
  - 21 sensors Ã— 100+ units Ã— 260+ cycles
  - Sensor types: temperature, pressure, vibration, speed, power
- **Synthetic maintenance logs** â€” Generated repair history, costs, downtime
- **Operational context** â€” Machine specs, crew schedules, production lines

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Train All Models

```bash
python scripts/train_all.py
```

This will:
- Download the C-MAPSS dataset (NASA S3 direct download, with Kaggle API fallback)
- Generate synthetic maintenance & operational data
- Engineer features (rolling stats, trends, regimes)
- Train LSTM Autoencoder, LSTM Predictor, XGBoost, and Bayesian Survival models
- Run Monte Carlo maintenance policy simulation

### 3. Run Inference Pipeline

```bash
python scripts/run_pipeline.py
```

### 4. Launch Dashboard

```bash
streamlit run dashboard/app.py
```

### 5. Run Unit Tests

```bash
python -m pytest tests/ -v
```

## ğŸ“ Project Structure

```
â”œâ”€â”€ config.py                    # Global configuration
â”œâ”€â”€ requirements.txt             # Dependencies
â”œâ”€â”€ PROJECT_REPORT.md            # Full project report
â”œâ”€â”€ .gitignore
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # NASA C-MAPSS dataset
â”‚   â”œâ”€â”€ processed/               # Windowed sequences
â”‚   â””â”€â”€ synthetic/               # Generated maintenance logs
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/                    # Data engineering pipeline
â”‚   â”‚   â”œâ”€â”€ download.py          # Multi-source dataset download
â”‚   â”‚   â”œâ”€â”€ preprocess.py        # Cleaning, normalization, windowing
â”‚   â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”‚   â””â”€â”€ synthetic_generator.py
â”‚   â”œâ”€â”€ models/                  # ML models
â”‚   â”‚   â”œâ”€â”€ autoencoder.py       # LSTM temporal autoencoder
â”‚   â”‚   â”œâ”€â”€ lstm_predictor.py    # Failure probability classifier
â”‚   â”‚   â”œâ”€â”€ xgboost_rul.py       # RUL regression
â”‚   â”‚   â””â”€â”€ bayesian_survival.py # Weibull survival analysis
â”‚   â”œâ”€â”€ explainability/          # Model interpretability
â”‚   â”‚   â”œâ”€â”€ shap_analysis.py     # SHAP feature attribution
â”‚   â”‚   â””â”€â”€ attention_viz.py     # Temporal attention heatmaps
â”‚   â”œâ”€â”€ optimization/            # Decision optimization
â”‚   â”‚   â””â”€â”€ milp_scheduler.py    # PuLP maintenance scheduler
â”‚   â””â”€â”€ evaluation/              # Evaluation & simulation
â”‚       â””â”€â”€ simulation.py        # Monte Carlo policy comparison
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_all.py             # End-to-end training
â”‚   â””â”€â”€ run_pipeline.py          # Full inference pipeline
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ app.py                   # Streamlit dashboard
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ capstone_colab.ipynb     # Google Colab notebook (GPU)
â””â”€â”€ tests/
    â”œâ”€â”€ test_preprocess.py
    â”œâ”€â”€ test_models.py
    â””â”€â”€ test_optimizer.py
```

## âš¡ GPU Support

The system automatically detects and uses CUDA GPUs for LSTM training:

```python
# config.py
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```

For Google Colab with GPU, use the notebook at `notebooks/capstone_colab.ipynb`.

## ğŸ“ˆ Results

| Model | Metric | Value |
|-------|--------|-------|
| LSTM Predictor | F1-Score | **0.933** |
| LSTM Predictor | AUC-ROC | **0.997** |
| XGBoost RUL | RMSE | **10.48 cycles** |
| XGBoost RUL | RÂ² | **0.937** |
| Bayesian Survival | C-Index | **0.992** |
| MILP Optimization | Cost Reduction | **97.4%** vs reactive |
| MILP Optimization | Downtime Reduction | **72.4%** vs reactive |

## ğŸ› ï¸ Maintenance Categories

| Level | Threshold | Action |
|-------|-----------|--------|
| ğŸ”´ Critical | Risk â‰¥ 70% | Service Immediately |
| ğŸŸ¡ Elevated | Risk 40-70% | Schedule Soon |
| ğŸŸ¢ Normal | Risk < 40% | Continue Monitoring |

## ğŸ“„ License

This project is developed for FSE 570 at Arizona State University.
