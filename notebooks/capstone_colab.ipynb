{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè≠ Smart Industrial Maintenance System ‚Äî GPU Training Notebook\n",
    "\n",
    "**FSE 570 Capstone** | Arizona State University\n",
    "\n",
    "This notebook runs the complete training pipeline on Google Colab with GPU acceleration.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.8.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\devas\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\devas\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mediapipe 0.10.21 requires numpy<2, but you have numpy 2.4.2 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: C:\\Users\\devas\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install -q xgboost lifelines shap pulp kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your project repo (UPDATE THE URL)\n",
    "!git clone https://github.com/YOUR_USERNAME/Capstone-Project.git\n",
    "%cd Capstone-Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Kaggle credentials\n",
    "# Upload your kaggle.json or set environment variables\n",
    "import os\n",
    "os.environ['KAGGLE_USERNAME'] = 'YOUR_KAGGLE_USERNAME'  # UPDATE\n",
    "os.environ['KAGGLE_KEY'] = 'YOUR_KAGGLE_KEY'  # UPDATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download & Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "from src.data.download import download_cmapss, load_cmapss_train\n",
    "from src.data.preprocess import DataPreprocessor\n",
    "from src.data.feature_engineering import FeatureEngineer\n",
    "from src.data.synthetic_generator import SyntheticDataGenerator\n",
    "\n",
    "# Download C-MAPSS dataset\n",
    "download_cmapss()\n",
    "df_train = load_cmapss_train()\n",
    "print(f\"Training data: {df_train.shape}\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "gen = SyntheticDataGenerator()\n",
    "logs, context, schedule = gen.generate_all(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering (for XGBoost)\n",
    "fe = FeatureEngineer()\n",
    "df_engineered = fe.engineer_features(df_train.copy())\n",
    "print(f\"Engineered features: {df_engineered.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipeline (for LSTM models)\n",
    "preprocessor = DataPreprocessor()\n",
    "data = preprocessor.fit_transform(df_train)\n",
    "preprocessor.save()\n",
    "\n",
    "import numpy as np\n",
    "for split_name, split_data in data.items():\n",
    "    np.savez_compressed(\n",
    "        os.path.join(config.PROCESSED_DATA_DIR, f\"{split_name}_data.npz\"),\n",
    "        **split_data\n",
    "    )\n",
    "\n",
    "X_train = data['train']['X']\n",
    "y_train_rul = data['train']['y_rul']\n",
    "y_train_binary = data['train']['y_binary']\n",
    "X_val = data['val']['X']\n",
    "y_val_binary = data['val']['y_binary']\n",
    "\n",
    "n_features = X_train.shape[2]\n",
    "print(f\"Sequences: {X_train.shape}, Features: {n_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train LSTM Autoencoder (Anomaly Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.autoencoder import LSTMAutoencoder, AutoencoderTrainer\n",
    "\n",
    "# Train on healthy data only\n",
    "healthy_mask = y_train_rul > config.MAX_RUL * 0.5\n",
    "X_healthy = X_train[healthy_mask]\n",
    "X_val_ae = X_val[data['val']['y_rul'] > config.MAX_RUL * 0.5]\n",
    "\n",
    "print(f\"Training autoencoder on {len(X_healthy)} healthy samples\")\n",
    "print(f\"Device: {config.DEVICE}\")\n",
    "\n",
    "autoencoder = LSTMAutoencoder(input_dim=n_features)\n",
    "ae_trainer = AutoencoderTrainer(autoencoder, epochs=50)\n",
    "ae_trainer.train(X_healthy, X_val_ae)\n",
    "ae_trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(ae_trainer.train_history, label='Train Loss', color='#3a7bd5')\n",
    "if ae_trainer.val_history:\n",
    "    ax.plot(ae_trainer.val_history, label='Val Loss', color='#FF6B6B')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('MSE Loss')\n",
    "ax.set_title('Autoencoder Training')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train LSTM Failure Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.lstm_predictor import LSTMPredictor, PredictorTrainer\n",
    "\n",
    "predictor = LSTMPredictor(input_dim=n_features)\n",
    "pred_trainer = PredictorTrainer(predictor, epochs=50)\n",
    "pred_trainer.train(X_train, y_train_binary, X_val, y_val_binary)\n",
    "pred_trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictor training\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "ax1.plot(pred_trainer.train_history, label='Train Loss', color='#3a7bd5')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Predictor Training Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "if pred_trainer.val_history:\n",
    "    epochs = range(5, len(pred_trainer.val_history) * 5 + 1, 5)\n",
    "    f1s = [m['f1'] for m in pred_trainer.val_history]\n",
    "    aucs = [m['auc'] for m in pred_trainer.val_history]\n",
    "    ax2.plot(epochs, f1s, label='F1 Score', color='#44BB44', marker='o')\n",
    "    ax2.plot(epochs, aucs, label='AUC', color='#FF6B6B', marker='s')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax2.set_title('Validation Metrics')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train XGBoost RUL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.xgboost_rul import XGBoostRUL\n",
    "\n",
    "exclude_cols = ['unit_id', 'cycle', 'RUL']\n",
    "feature_cols = [c for c in df_engineered.columns if c not in exclude_cols]\n",
    "\n",
    "unit_ids = df_engineered['unit_id'].unique()\n",
    "np.random.seed(config.RANDOM_SEED)\n",
    "np.random.shuffle(unit_ids)\n",
    "n = len(unit_ids)\n",
    "train_units = unit_ids[:int(n * 0.7)]\n",
    "val_units = unit_ids[int(n * 0.7):int(n * 0.85)]\n",
    "\n",
    "X_train_xgb = df_engineered[df_engineered['unit_id'].isin(train_units)][feature_cols]\n",
    "y_train_xgb = df_engineered[df_engineered['unit_id'].isin(train_units)]['RUL'].values\n",
    "X_val_xgb = df_engineered[df_engineered['unit_id'].isin(val_units)][feature_cols]\n",
    "y_val_xgb = df_engineered[df_engineered['unit_id'].isin(val_units)]['RUL'].values\n",
    "\n",
    "xgb_model = XGBoostRUL()\n",
    "xgb_model.train(X_train_xgb, y_train_xgb, X_val_xgb, y_val_xgb)\n",
    "xgb_model.evaluate(X_val_xgb, y_val_xgb)\n",
    "xgb_model.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bayesian Survival Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.bayesian_survival import BayesianSurvival\n",
    "\n",
    "survival_features = config.ACTIVE_SENSORS + ['cycle']\n",
    "survival_cols = [c for c in survival_features if c in df_train.columns] + ['RUL']\n",
    "\n",
    "df_survival_train = df_train[df_train['unit_id'].isin(train_units)][['unit_id'] + survival_cols]\n",
    "\n",
    "survival_model = BayesianSurvival()\n",
    "survival_model.fit(df_survival_train)\n",
    "\n",
    "df_survival_val = df_train[df_train['unit_id'].isin(val_units)][['unit_id'] + survival_cols]\n",
    "survival_model.evaluate(df_survival_val)\n",
    "survival_model.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Explainability (SHAP & Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.explainability.shap_analysis import SHAPExplainer\n",
    "from src.explainability.attention_viz import AttentionVisualizer\n",
    "\n",
    "# SHAP for XGBoost\n",
    "shap_explainer = SHAPExplainer(xgb_model, model_type='xgboost')\n",
    "shap_explainer.compute_shap_values(X_val_xgb)\n",
    "shap_explainer.plot_global_importance(save_path='shap_importance.png')\n",
    "shap_explainer.plot_beeswarm(save_path='shap_beeswarm.png')\n",
    "ranking = shap_explainer.get_sensor_ranking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention visualization\n",
    "from src.models.lstm_predictor import load_predictor\n",
    "loaded_predictor = load_predictor()\n",
    "\n",
    "attn_viz = AttentionVisualizer(loaded_predictor)\n",
    "attn_viz.plot_attention_heatmap(data['test']['X'], save_path='attention_heatmap.png')\n",
    "attn_viz.plot_average_attention(data['test']['X'], data['test']['y_binary'],\n",
    "                                 save_path='attention_comparison.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. MILP Optimization & Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.optimization.milp_scheduler import MaintenanceScheduler\n",
    "\n",
    "# Get predictions for test data\n",
    "failure_proba, _ = loaded_predictor.predict_proba(torch.FloatTensor(data['test']['X']))\n",
    "\n",
    "# Aggregate per unit\n",
    "unit_risks = {}\n",
    "for uid in np.unique(data['test']['unit_ids']):\n",
    "    mask = data['test']['unit_ids'] == uid\n",
    "    unit_risks[int(uid)] = float(failure_proba[mask][-1])\n",
    "\n",
    "# Run MILP optimization\n",
    "scheduler = MaintenanceScheduler()\n",
    "result = scheduler.create_schedule(\n",
    "    machine_risks=unit_risks,\n",
    "    machine_names={uid: f'Engine-{uid:03d}' for uid in unit_risks}\n",
    ")\n",
    "result['schedule']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo simulation\n",
    "from src.evaluation.simulation import MaintenanceSimulator\n",
    "\n",
    "sim = MaintenanceSimulator(n_machines=50, n_periods=100)\n",
    "sim_df, sim_summary = sim.run_comparison(n_simulations=100)\n",
    "sim.plot_comparison(sim_df, save_path='simulation_comparison.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Download Results\n",
    "\n",
    "Download trained models and results back to local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results to a zip for download\n",
    "import shutil\n",
    "shutil.make_archive('capstone_results', 'zip', '.', 'models/saved')\n",
    "\n",
    "# In Colab, download the zip:\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('capstone_results.zip')\n",
    "except ImportError:\n",
    "    print('Not in Colab. Files saved locally.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Project**: FSE 570 Data Science Capstone | Arizona State University"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
