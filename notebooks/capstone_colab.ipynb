{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ­ Smart Industrial Maintenance System â€” GPU Training Notebook\n",
                "\n",
                "**FSE 570 Capstone** | Arizona State University\n",
                "\n",
                "This notebook runs the complete training pipeline on Google Colab with GPU acceleration.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup Environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPU availability\n",
                "import torch\n",
                "print(f\"PyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    props = torch.cuda.get_device_properties(0)\n",
                "    vram = getattr(props, 'total_memory', getattr(props, 'total_mem', 0))\n",
                "    print(f\"VRAM: {vram / 1e9:.1f} GB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q xgboost lifelines shap pulp"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone your project repo\n",
                "!git clone https://github.com/SivaKanth007/Capstone-Project.git\n",
                "%cd Capstone-Project"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Download & Preprocess Data\n",
                "\n",
                "The download module will automatically try direct download first â€” **no Kaggle credentials needed**.\n",
                "If the direct download fails, it falls back to a synthetic data generator."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import config\n",
                "from src.data.download import download_cmapss, load_cmapss_train\n",
                "from src.data.preprocess import DataPreprocessor\n",
                "from src.data.feature_engineering import FeatureEngineer\n",
                "from src.data.synthetic_generator import SyntheticDataGenerator\n",
                "\n",
                "# Download C-MAPSS dataset (direct URL, no auth)\n",
                "download_cmapss()\n",
                "df_train = load_cmapss_train()\n",
                "print(f\"\\nTraining data: {df_train.shape}\")\n",
                "df_train.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate synthetic data\n",
                "gen = SyntheticDataGenerator()\n",
                "logs, context, schedule = gen.generate_all(df_train)\n",
                "print(f\"Maintenance logs: {logs.shape}\")\n",
                "print(f\"Operational context: {context.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature engineering (for XGBoost)\n",
                "fe = FeatureEngineer()\n",
                "df_engineered = fe.engineer_features(df_train.copy())\n",
                "print(f\"Engineered features: {df_engineered.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preprocessing pipeline (for LSTM models)\n",
                "preprocessor = DataPreprocessor()\n",
                "data = preprocessor.fit_transform(df_train)\n",
                "preprocessor.save()\n",
                "\n",
                "for split_name, split_data in data.items():\n",
                "    np.savez_compressed(\n",
                "        os.path.join(config.PROCESSED_DATA_DIR, f\"{split_name}_data.npz\"),\n",
                "        **split_data\n",
                "    )\n",
                "\n",
                "X_train = data['train']['X']\n",
                "y_train_rul = data['train']['y_rul']\n",
                "y_train_binary = data['train']['y_binary']\n",
                "X_val = data['val']['X']\n",
                "y_val_binary = data['val']['y_binary']\n",
                "\n",
                "n_features = X_train.shape[2]\n",
                "print(f\"Sequences: {X_train.shape}, Features: {n_features}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Train LSTM Autoencoder (Anomaly Detection)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.models.autoencoder import LSTMAutoencoder, AutoencoderTrainer\n",
                "\n",
                "# Train on healthy data only\n",
                "healthy_mask = y_train_rul > config.MAX_RUL * 0.5\n",
                "X_healthy = X_train[healthy_mask]\n",
                "X_val_ae = X_val[data['val']['y_rul'] > config.MAX_RUL * 0.5]\n",
                "\n",
                "print(f\"Training autoencoder on {len(X_healthy)} healthy samples\")\n",
                "print(f\"Device: {config.DEVICE}\")\n",
                "\n",
                "autoencoder = LSTMAutoencoder(input_dim=n_features)\n",
                "ae_trainer = AutoencoderTrainer(autoencoder, epochs=50)\n",
                "ae_trainer.train(X_healthy, X_val_ae)\n",
                "ae_trainer.save_model()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize training loss\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 4))\n",
                "ax.plot(ae_trainer.train_history, label='Train Loss', color='#3a7bd5')\n",
                "if ae_trainer.val_history:\n",
                "    ax.plot(ae_trainer.val_history, label='Val Loss', color='#FF6B6B')\n",
                "ax.set_xlabel('Epoch')\n",
                "ax.set_ylabel('MSE Loss')\n",
                "ax.set_title('Autoencoder Training')\n",
                "ax.legend()\n",
                "ax.grid(True, alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Train LSTM Failure Predictor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.models.lstm_predictor import LSTMPredictor, PredictorTrainer\n",
                "\n",
                "predictor = LSTMPredictor(input_dim=n_features)\n",
                "pred_trainer = PredictorTrainer(predictor, epochs=50)\n",
                "pred_trainer.train(X_train, y_train_binary, X_val, y_val_binary)\n",
                "pred_trainer.save_model()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize predictor training\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
                "\n",
                "ax1.plot(pred_trainer.train_history, label='Train Loss', color='#3a7bd5')\n",
                "ax1.set_xlabel('Epoch')\n",
                "ax1.set_ylabel('Loss')\n",
                "ax1.set_title('Predictor Training Loss')\n",
                "ax1.legend()\n",
                "ax1.grid(True, alpha=0.3)\n",
                "\n",
                "if pred_trainer.val_history:\n",
                "    epochs_list = range(5, len(pred_trainer.val_history) * 5 + 1, 5)\n",
                "    f1s = [m.get('f1', 0) for m in pred_trainer.val_history]\n",
                "    aucs = [m.get('auc', 0) for m in pred_trainer.val_history]\n",
                "    ax2.plot(epochs_list, f1s, label='F1 Score', color='#44BB44', marker='o', markersize=4)\n",
                "    ax2.plot(epochs_list, aucs, label='AUC', color='#FF6B6B', marker='s', markersize=4)\n",
                "    ax2.set_xlabel('Epoch')\n",
                "    ax2.set_ylabel('Score')\n",
                "    ax2.set_title('Validation Metrics')\n",
                "    ax2.legend()\n",
                "    ax2.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Train XGBoost RUL Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.models.xgboost_rul import XGBoostRUL\n",
                "\n",
                "exclude_cols = ['unit_id', 'cycle', 'RUL']\n",
                "feature_cols = [c for c in df_engineered.columns if c not in exclude_cols]\n",
                "\n",
                "unit_ids = df_engineered['unit_id'].unique()\n",
                "np.random.seed(config.RANDOM_SEED)\n",
                "np.random.shuffle(unit_ids)\n",
                "n = len(unit_ids)\n",
                "train_units = unit_ids[:int(n * 0.7)]\n",
                "val_units = unit_ids[int(n * 0.7):int(n * 0.85)]\n",
                "\n",
                "X_train_xgb = df_engineered[df_engineered['unit_id'].isin(train_units)][feature_cols]\n",
                "y_train_xgb = df_engineered[df_engineered['unit_id'].isin(train_units)]['RUL'].values\n",
                "X_val_xgb = df_engineered[df_engineered['unit_id'].isin(val_units)][feature_cols]\n",
                "y_val_xgb = df_engineered[df_engineered['unit_id'].isin(val_units)]['RUL'].values\n",
                "\n",
                "xgb_model = XGBoostRUL()\n",
                "xgb_model.train(X_train_xgb, y_train_xgb, X_val_xgb, y_val_xgb,\n",
                "                feature_names=feature_cols)\n",
                "xgb_model.evaluate(X_val_xgb, y_val_xgb)\n",
                "xgb_model.save()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Bayesian Survival Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.models.bayesian_survival import BayesianSurvival\n",
                "\n",
                "survival_features = config.ACTIVE_SENSORS + ['cycle']\n",
                "survival_cols = [c for c in survival_features if c in df_train.columns] + ['RUL']\n",
                "\n",
                "df_survival_train = df_train[df_train['unit_id'].isin(train_units)][['unit_id'] + survival_cols]\n",
                "\n",
                "survival_model = BayesianSurvival()\n",
                "survival_model.fit(df_survival_train)\n",
                "\n",
                "df_survival_val = df_train[df_train['unit_id'].isin(val_units)][['unit_id'] + survival_cols]\n",
                "survival_model.evaluate(df_survival_val)\n",
                "survival_model.save()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Explainability (SHAP & Attention)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.explainability.shap_analysis import SHAPExplainer\n",
                "\n",
                "# SHAP for XGBoost\n",
                "shap_explainer = SHAPExplainer(xgb_model, model_type='xgboost')\n",
                "shap_explainer.compute_shap_values(X_val_xgb)\n",
                "shap_explainer.plot_global_importance(save_path='shap_importance.png')\n",
                "shap_explainer.plot_beeswarm(save_path='shap_beeswarm.png')\n",
                "ranking = shap_explainer.get_sensor_ranking()\n",
                "ranking.head(15)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.explainability.attention_viz import AttentionVisualizer\n",
                "\n",
                "# Attention visualization\n",
                "attn_viz = AttentionVisualizer(predictor)\n",
                "attn_viz.plot_attention_heatmap(data['test']['X'], save_path='attention_heatmap.png')\n",
                "attn_viz.plot_average_attention(data['test']['X'], data['test']['y_binary'],\n",
                "                                save_path='attention_comparison.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. MILP Optimization & Simulation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.optimization.milp_scheduler import MaintenanceScheduler\n",
                "\n",
                "# Get predictions for test data\n",
                "failure_proba, _ = predictor.predict_proba(torch.FloatTensor(data['test']['X']).to(config.DEVICE))\n",
                "\n",
                "# Aggregate per unit\n",
                "unit_risks = {}\n",
                "for uid in np.unique(data['test']['unit_ids']):\n",
                "    mask = data['test']['unit_ids'] == uid\n",
                "    unit_risks[int(uid)] = float(failure_proba[mask][-1])\n",
                "\n",
                "# Run MILP optimization\n",
                "scheduler = MaintenanceScheduler()\n",
                "result = scheduler.create_schedule(\n",
                "    machine_risks=unit_risks,\n",
                "    machine_names={uid: f'Engine-{uid:03d}' for uid in unit_risks}\n",
                ")\n",
                "result['schedule']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Monte Carlo simulation\n",
                "from src.evaluation.simulation import MaintenanceSimulator\n",
                "\n",
                "sim = MaintenanceSimulator(n_machines=50, n_periods=100)\n",
                "sim_df, sim_summary = sim.run_comparison(n_simulations=100)\n",
                "sim.plot_comparison(sim_df, save_path='simulation_comparison.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Download Results\n",
                "\n",
                "Download trained models and results back to local machine."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save all results to a zip for download\n",
                "import shutil\n",
                "shutil.make_archive('capstone_results', 'zip', '.', 'models/saved')\n",
                "\n",
                "# In Colab, download the zip:\n",
                "try:\n",
                "    from google.colab import files\n",
                "    files.download('capstone_results.zip')\n",
                "except ImportError:\n",
                "    print('Not in Colab. Files saved locally.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "**Project**: FSE 570 Data Science Capstone | Arizona State University"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}